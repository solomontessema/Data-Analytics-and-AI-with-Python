{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solomontessema/Data-Analytics-and-AI-with-Python/blob/main/notebooks/sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "id": "title-intro",
      "cell_type": "markdown",
      "metadata": {
        "id": "title-intro"
      },
      "source": [
        "# Airbnb Reviews with PySpark (Colab)\n",
        "\n",
        "\n",
        "1. Sets up Java & Spark on Colab\n",
        "2. Loads the Airbnb listings dataset from Google Drive\n",
        "3. Performs light cleaning and exploratory aggregations\n",
        "4. Builds a simple price prediction model (Linear Regression)\n",
        "5. Saves the cleaned data and model, then runs a sample prediction"
      ]
    },
    {
      "id": "connect-drive",
      "cell_type": "markdown",
      "metadata": {
        "id": "connect-drive"
      },
      "source": [
        "## 1) Connect Google Drive\n",
        "Mount your Drive so we can read/write datasets and models."
      ]
    },
    {
      "id": "code-mount-drive",
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "code-mount-drive"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "id": "install-java-spark",
      "cell_type": "markdown",
      "metadata": {
        "id": "install-java-spark"
      },
      "source": [
        "## 2) Install Java & Spark\n",
        "Install OpenJDK 11, download Spark 3.4.1 (Hadoop 3), extract it, and install `findspark`."
      ]
    },
    {
      "id": "code-install-java-spark",
      "cell_type": "code",
      "metadata": {
        "id": "code-install-java-spark"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://mirrors.huaweicloud.com/apache/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar -xvf spark-3.4.1-bin-hadoop3.tgz > /dev/null\n",
        "!pip install -q findspark"
      ]
    },
    {
      "id": "init-spark",
      "cell_type": "markdown",
      "metadata": {
        "id": "init-spark"
      },
      "source": [
        "## 3) Initialize Spark\n",
        "Set environment variables, initialize `findspark`, and start a `SparkSession`."
      ]
    },
    {
      "id": "code-init-spark",
      "cell_type": "code",
      "metadata": {
        "id": "code-init-spark"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"AirbnbReviews\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "id": "load-data",
      "cell_type": "markdown",
      "metadata": {
        "id": "load-data"
      },
      "source": [
        "## 4) Load Data\n",
        "Read the CSV from Drive. Update the path if your file is located elsewhere. We display the schema and a few rows to verify."
      ]
    },
    {
      "id": "code-load-data",
      "cell_type": "code",
      "metadata": {
        "id": "code-load-data"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/MyDrive/listings.csv\"\n",
        "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "df.printSchema()\n",
        "df.show(5, truncate=False)"
      ]
    },
    {
      "id": "clean-transform",
      "cell_type": "markdown",
      "metadata": {
        "id": "clean-transform"
      },
      "source": [
        "## 5) Light Cleaning & Transformation\n",
        "- Keep rows with non-null `price`\n",
        "- Parse `last_review` to a proper date type\n",
        "- Preview selected columns"
      ]
    },
    {
      "id": "code-clean-transform",
      "cell_type": "code",
      "metadata": {
        "id": "code-clean-transform"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, to_date\n",
        "\n",
        "df_clean = df.filter(col(\"price\").isNotNull())\n",
        "df_clean = df_clean.withColumn(\"last_review\", to_date(col(\"last_review\")))\n",
        "df_clean.select(\"id\", \"name\", \"price\", \"last_review\").show(10, truncate=False)"
      ]
    },
    {
      "id": "eda-aggregations",
      "cell_type": "markdown",
      "metadata": {
        "id": "eda-aggregations"
      },
      "source": [
        "## 6) Exploratory Aggregations\n",
        "A couple of simple summaries:\n",
        "- Average price by `room_type`\n",
        "- Top neighborhoods by listing count"
      ]
    },
    {
      "id": "code-avg-price-roomtype",
      "cell_type": "code",
      "metadata": {
        "id": "code-avg-price-roomtype"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "df_clean.groupBy(\"room_type\").avg(\"price\").orderBy(\"avg(price)\", ascending=False).show(truncate=False)"
      ]
    },
    {
      "id": "code-top-neighborhoods",
      "cell_type": "code",
      "metadata": {
        "id": "code-top-neighborhoods"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "df_clean.groupBy(\"neighbourhood\").count().orderBy(\"count\", ascending=False).show(20, truncate=False)"
      ]
    },
    {
      "id": "save-cleaned",
      "cell_type": "markdown",
      "metadata": {
        "id": "save-cleaned"
      },
      "source": [
        "## 7) Save Cleaned Data\n",
        "Store the cleaned dataset to Parquet (columnar format) for efficient future use."
      ]
    },
    {
      "id": "code-save-cleaned",
      "cell_type": "code",
      "metadata": {
        "id": "code-save-cleaned"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "clean_out_path = \"/content/drive/MyDrive/listings_clean.parquet\"\n",
        "df_clean.write.mode(\"overwrite\").parquet(clean_out_path)\n",
        "print(f\"Saved cleaned data to: {clean_out_path}\")"
      ]
    },
    {
      "id": "ml-intro",
      "cell_type": "markdown",
      "metadata": {
        "id": "ml-intro"
      },
      "source": [
        "## 8) Build a Simple Price Prediction Model\n",
        "We train a **Linear Regression** model to predict `price` using:\n",
        "- Encoded `room_type`\n",
        "- `minimum_nights`\n",
        "- `number_of_reviews`\n",
        "\n",
        "Steps:\n",
        "1. Drop rows missing required fields\n",
        "2. Cast numeric columns to `double`\n",
        "3. Encode `room_type` using `StringIndexer`\n",
        "4. Assemble features into a single vector\n",
        "5. Train/Test split and model training\n",
        "6. Evaluate with RMSE"
      ]
    },
    {
      "id": "code-train-model",
      "cell_type": "code",
      "metadata": {
        "id": "code-train-model"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, isnan\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Prepare ML dataframe\n",
        "ml_df = df.dropna(subset=[\"room_type\", \"minimum_nights\", \"number_of_reviews\", \"price\"])\n",
        "ml_df = ml_df.withColumn(\"number_of_reviews\", col(\"number_of_reviews\").cast(\"double\"))\n",
        "ml_df = ml_df.withColumn(\"minimum_nights\", col(\"minimum_nights\").cast(\"double\"))\n",
        "ml_df = ml_df.withColumn(\"price\", col(\"price\").cast(\"double\"))\n",
        "ml_df = ml_df.filter((~isnan(\"price\")) & (col(\"price\").isNotNull()))\n",
        "\n",
        "# Encode room_type\n",
        "indexer = StringIndexer(inputCol=\"room_type\", outputCol=\"room_type_index\", handleInvalid=\"skip\")\n",
        "df_indexed = indexer.fit(ml_df).transform(ml_df)\n",
        "\n",
        "df_indexed = df_indexed.filter(\n",
        "    (col(\"minimum_nights\").isNotNull()) &\n",
        "    (col(\"number_of_reviews\").isNotNull()) &\n",
        "    (col(\"room_type_index\").isNotNull()) &\n",
        "    (col(\"price\").isNotNull())\n",
        ")\n",
        "\n",
        "\n",
        "# Assemble features\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"room_type_index\", \"minimum_nights\", \"number_of_reviews\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "df_vector = assembler.transform(df_indexed)\n",
        "df_vector = df_vector.filter(col(\"features\").isNotNull())\n",
        "\n",
        "# Train/test split\n",
        "train_df, test_df = df_vector.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
        "model = lr.fit(train_df)\n",
        "\n",
        "# Evaluate\n",
        "predictions = model.transform(test_df)\n",
        "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"Coefficients: {model.coefficients}\")\n",
        "print(f\"Intercept: {model.intercept:.4f}\")"
      ]
    },
    {
      "id": "save-reload-model",
      "cell_type": "markdown",
      "metadata": {
        "id": "save-reload-model"
      },
      "source": [
        "## 9) Save & Reload the Model\n",
        "Persist the trained model to Drive and show how to load it back for later use."
      ]
    },
    {
      "id": "code-save-reload-model",
      "cell_type": "code",
      "metadata": {
        "id": "code-save-reload-model"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/linear_model\"\n",
        "\n",
        "# uncomment the following to save the model if not saved already.\n",
        "# model.save(model_path)\n",
        "\n",
        "from pyspark.ml.regression import LinearRegressionModel\n",
        "loaded_model = LinearRegressionModel.load(model_path)\n",
        "print(\"Model reloaded.\")"
      ]
    },
    {
      "id": "sample-prediction",
      "cell_type": "markdown",
      "metadata": {
        "id": "sample-prediction"
      },
      "source": [
        "## 10) Make a Sample Prediction\n",
        "Predict price for a synthetic listing described by:\n",
        "- `room_type_index = 1.0`\n",
        "- `minimum_nights = 3.0`\n",
        "- `number_of_reviews = 25.0`\n",
        "\n",
        "> In a production workflow, map real `room_type` strings to indices using the fitted `StringIndexerModel`."
      ]
    },
    {
      "id": "code-sample-prediction",
      "cell_type": "code",
      "metadata": {
        "id": "code-sample-prediction"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql import Row\n",
        "\n",
        "sample_data = [Row(features=Vectors.dense([1.0, 3.0, 25.0]))]\n",
        "sample_df = spark.createDataFrame(sample_data)\n",
        "prediction = loaded_model.transform(sample_df)\n",
        "prediction.select(\"features\", \"prediction\").show(truncate=False)"
      ]
    },
    {
      "id": "next-steps",
      "cell_type": "markdown",
      "metadata": {
        "id": "next-steps"
      },
      "source": [
        "---\n",
        "### Next Steps\n",
        "- Feature engineering (e.g., host features, location encodings)\n",
        "- Robust outlier handling for `price`\n",
        "- Try tree-based models (Random Forest, GBT) and compare metrics\n",
        "- Cross-validation and hyperparameter tuning for better generalization"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}