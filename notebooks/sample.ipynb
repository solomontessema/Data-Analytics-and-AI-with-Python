{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
   "kernelspec": {
     "name": "python3",
     "display_name": "Python 3"
   },
   "language_info": {
     "name": "python"
   }
 },
 "cells": [
   {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
       "# 🏡 Airbnb Data Analysis with PySpark\n",
       "This notebook demonstrates how to load, clean, and analyze Airbnb listings using PySpark. We'll walk through:\n",
       "- Mounting Google Drive\n",
       "- Installing Spark\n",
       "- Loading CSV data\n",
       "- Cleaning and transforming\n",
       "- Running basic aggregations\n",
       "- Training a linear regression model"
     ]
   },
   {
     "cell_type": "code",
     "metadata": {},
     "source": [
       "# Mount Google Drive\n",
       "from google.colab import drive\n",
       "drive.mount('/content/drive')"
     ]
   },
   {
     "cell_type": "code",
     "metadata": {},
     "source": [
       "# Install Spark\n",
       "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
       "!wget -q https://mirrors.huaweicloud.com/apache/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
       "!tar -xvf spark-3.4.1-bin-hadoop3.tgz\n",
       "!pip install -q findspark"
     ]
   },
   {
     "cell_type": "code",
     "metadata": {},
     "source": [
       "# Configure environment\n",
       "import os\n",
       "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
       "os.environ['SPARK_HOME'] = '/content/spark-3.4.1-bin-hadoop3'\n",
       "import findspark\n",
       "findspark.init()\n",
       "from pyspark.sql import SparkSession\n",
       "spark = SparkSession.builder.appName('AirbnbReviews').getOrCreate()"
     ]
   },
   {
     "cell_type": "code",
     "metadata": {},
     "source": [
       "# Load dataset\n",
       "df = spark.read.csv('/content/drive/MyDrive/listings.csv', header=True, inferSchema=True)\n",
       "df.printSchema()\n",
       "df.show(5)"
     ]
   },
   {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
       "## 🧹 Data Cleaning\n",
       "We'll filter out null prices and convert review dates to proper date format."
     ]
   },
   {
     "cell_type": "code",
     "metadata": {},
     "source": [
       "from pyspark.sql.functions import col, to_date\n",
       "df_clean = df.filter(col('price').isNotNull())\n",
       "df_clean = df_clean.withColumn('last_review', to_date(col('last_review')))\n",
       "df_clean.select('id', 'name', 'price', 'last_review').show()"
     ]
   },
   {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
       "## 📊 Aggregation Examples\n",
       "Let's explore average price by room type and listing count by neighborhood."
     ]
   },
   {
     "cell_type": "code",
     "metadata": {},
     "source": [
       "df_clean.groupBy('room_type').avg('price').orderBy('avg(price)', ascending=False).show()"
     ]
   },
   {
     "cell_type": "code",
     "metadata": {},
     "source": [
       "df_clean.groupBy('neighbourhood').count().orderBy('count', ascending=False).show()"
     ]
   }
 ]
}
