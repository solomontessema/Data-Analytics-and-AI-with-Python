{
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Airbnb Reviews with PySpark (Colab)\n",
"\n",
"A concise, pedagogically structured notebook that:\n",
"\n",
"1. Sets up Java & Spark on Colab\n",
"2. Loads the Airbnb listings dataset from Google Drive\n",
"3. Performs light cleaning and exploratory aggregations\n",
"4. Builds a simple price prediction model using Linear Regression\n",
"5. Saves the cleaned data and trained model, then runs a sample prediction"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 1) Connect Google Drive\n",
"Mount your Drive so we can read/write datasets and models."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"cellView": "form"
},
"outputs": [],
"source": [
"from google.colab import drive\n",
"drive.mount('/content/drive/')"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 2) Install Java & Spark\n",
"This installs OpenJDK 11, downloads Spark 3.4.1 (Hadoop 3 build), extracts it, and installs `findspark`."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Install Java\n",
"!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
"\n",
"# Download Spark (3.4.1) and extract\n",
"!wget -q [https://mirrors.huaweicloud.com/apache/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n](https://mirrors.huaweicloud.com/apache/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n)",
"!tar -xvf spark-3.4.1-bin-hadoop3.tgz > /dev/null\n",
"\n",
"# Install findspark to help Python find the local Spark install\n",
"!pip install -q findspark"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 3) Initialize Spark\n",
"Set environment variables, initialize `findspark`, and start a SparkSession."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"import os\n",
"os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"\n",
"os.environ["SPARK_HOME"] = "/content/spark-3.4.1-bin-hadoop3"\n",
"\n",
"import findspark\n",
"findspark.init()\n",
"\n",
"from pyspark.sql import SparkSession\n",
"spark = SparkSession.builder.appName("AirbnbReviews").getOrCreate()\n",
"spark"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 4) Load Data\n",
"Read the CSV from Drive. Update the path if your file is located elsewhere. We display the schema and a few rows to verify."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"data_path = "/content/drive/MyDrive/listings.csv"\n",
"df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
"df.printSchema()\n",
"df.show(5, truncate=False)"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 5) Light Cleaning & Transformation\n",
"- Keep rows with non-null `price`\n",
"- Parse `last_review` to a proper date type\n",
"- Preview selected columns"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"from pyspark.sql.functions import col, to_date\n",
"\n",
"df_clean = df.filter(col("price").isNotNull())\n",
"df_clean = df_clean.withColumn("last_review", to_date(col("last_review")))\n",
"df_clean.select("id", "name", "price", "last_review").show(10, truncate=False)"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 6) Exploratory Aggregations\n",
"A couple of simple summaries:\n",
"- Average price by `room_type`\n",
"- Top neighborhoods by listing count"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"df_clean.groupBy("room_type").avg("price").orderBy("avg(price)", ascending=False).show(truncate=False)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"df_clean.groupBy("neighbourhood").count().orderBy("count", ascending=False).show(20, truncate=False)"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 7) Save Cleaned Data\n",
"Store the cleaned dataset to Parquet (columnar format) for efficient future use."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"clean_out_path = "/content/drive/MyDrive/listings_clean.parquet"\n",
"df_clean.write.mode("overwrite").parquet(clean_out_path)\n",
"print(f"Saved cleaned data to: {clean_out_path}")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 8) Build a Simple Price Prediction Model\n",
"We train a **Linear Regression** model to predict `price` using:\n",
"- Encoded `room_type`\n",
"- `minimum_nights`\n",
"- `number_of_reviews`\n",
"\n",
"Steps:\n",
"1. Drop rows missing required fields\n",
"2. Cast numeric columns to `double`\n",
"3. Encode `room_type` using `StringIndexer`\n",
"4. Assemble features into a single vector\n",
"5. Train/Test split and model training\n",
"6. Evaluate with RMSE"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"from pyspark.sql.functions import isnan\n",
"from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
"from pyspark.ml.regression import LinearRegression\n",
"from pyspark.ml.evaluation import RegressionEvaluator\n",
"\n",
"# Start from the original df to ensure full columns are available\n",
"ml_df = df.dropna(subset=["room_type", "minimum_nights", "number_of_reviews", "price"])\\n",
"           .withColumn("number_of_reviews", col("number_of_reviews").cast("double"))\\n",
"           .withColumn("minimum_nights", col("minimum_nights").cast("double"))\\n",
"           .withColumn("price", col("price").cast("double"))\n",
"\n",
"ml_df = ml_df.filter((~isnan("price")) & (col("price").isNotNull()))\n",
"\n",
"indexer = StringIndexer(inputCol="room_type", outputCol="room_type_index", handleInvalid="skip")\n",
"df_indexed = indexer.fit(ml_df).transform(ml_df)\n",
"\n",
"assembler = VectorAssembler(\n",
"    inputCols=["room_type_index", "minimum_nights", "number_of_reviews"],\n",
"    outputCol="features"\n",
")\n",
"df_vector = assembler.transform(df_indexed).filter(col("features").isNotNull())\n",
"\n",
"train_df, test_df = df_vector.randomSplit([0.8, 0.2], seed=42)\n",
"\n",
"lr = LinearRegression(featuresCol="features", labelCol="price")\n",
"model = lr.fit(train_df)\n",
"predictions = model.transform(test_df)\n",
"\n",
"evaluator = RegressionEvaluator(labelCol="price", predictionCol="prediction", metricName="rmse")\n",
"rmse = evaluator.evaluate(predictions)\n",
"print(f"RMSE: {rmse:.4f}")\n",
"print(f"Coefficients: {model.coefficients}")\n",
"print(f"Intercept: {model.intercept:.4f}")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 9) Save & Reload the Model\n",
"Persist the trained model to Drive and show how to load it back for later use."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"model_path = "/content/drive/MyDrive/linear_model"\n",
"model.save(model_path)\n",
"print(f"Model saved to: {model_path}")\n",
"\n",
"from pyspark.ml.regression import LinearRegressionModel\n",
"loaded_model = LinearRegressionModel.load(model_path)\n",
"print("Model reloaded.")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 10) Make a Sample Prediction\n",
"Predict price for a synthetic listing described by:\n",
"- `room_type_index = 1.0`\n",
"- `minimum_nights = 3.0`\n",
"- `number_of_reviews = 25.0`\n",
"\n",
"> Note: In a production workflow, you would map real `room_type` strings to their index using the fitted `StringIndexerModel`."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"from pyspark.ml.linalg import Vectors\n",
"from pyspark.sql import Row\n",
"\n",
"sample_data = [Row(features=Vectors.dense([1.0, 3.0, 25.0]))]\n",
"sample_df = spark.createDataFrame(sample_data)\n",
"prediction = loaded_model.transform(sample_df)\n",
"prediction.select("features", "prediction").show(truncate=False)"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"---\n",
"### Next Steps\n",
"- Feature engineering (e.g., host features, location encodings)\n",
"- Robust outlier handling for `price`\n",
"- Try tree-based models (Random Forest, GBT) and compare metrics\n",
"- Cross-validation and hyperparameter tuning for better generalization"
]
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"name": "python",
"version": ""
}
},
"nbformat": 4,
"nbformat_minor": 5
}
